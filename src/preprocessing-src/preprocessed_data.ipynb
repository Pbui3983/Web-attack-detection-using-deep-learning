{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a16a13a7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== BẮT ĐẦU PIPELINE XỬ LÝ DỮ LIỆU ===\n",
      "Đang tải dữ liệu theo chunk...\n",
      "Đã tải chunk 1 (500000 mẫu) từ benign\n",
      "Đã tải chunk 2 (325187 mẫu) từ benign\n",
      "Đã tải chunk 1 (500000 mẫu) từ bruteforce_http\n",
      "Đã tải chunk 2 (412503 mẫu) từ bruteforce_http\n",
      "Đã tải chunk 1 (500000 mẫu) từ bruteforce_https\n",
      "Đã tải chunk 2 (365126 mẫu) từ bruteforce_https\n",
      "Đã tải chunk 1 (86443 mẫu) từ dos_http\n",
      "Đã tải chunk 1 (33216 mẫu) từ dos_https\n",
      "Đã tải chunk 1 (468275 mẫu) từ ftp_login\n",
      "Đã tải chunk 1 (11688 mẫu) từ ftp_version\n",
      "Đã tải chunk 1 (500000 mẫu) từ hostsweep_Pn\n",
      "Đã tải chunk 2 (500000 mẫu) từ hostsweep_Pn\n",
      "Đã tải chunk 3 (500000 mẫu) từ hostsweep_Pn\n",
      "Đã tải chunk 4 (500000 mẫu) từ hostsweep_Pn\n",
      "Đã tải chunk 5 (500000 mẫu) từ hostsweep_Pn\n",
      "Đã tải chunk 6 (500000 mẫu) từ hostsweep_Pn\n",
      "Đã tải chunk 7 (492290 mẫu) từ hostsweep_Pn\n",
      "Đã tải chunk 1 (22637 mẫu) từ hostsweep_sn\n",
      "Đã tải chunk 1 (500000 mẫu) từ portscan\n",
      "Đã tải chunk 2 (500000 mẫu) từ portscan\n",
      "Đã tải chunk 3 (500000 mẫu) từ portscan\n",
      "Đã tải chunk 4 (500000 mẫu) từ portscan\n",
      "Đã tải chunk 5 (500000 mẫu) từ portscan\n",
      "Đã tải chunk 6 (500000 mẫu) từ portscan\n",
      "Đã tải chunk 7 (500000 mẫu) từ portscan\n",
      "Đã tải chunk 8 (500000 mẫu) từ portscan\n",
      "Đã tải chunk 9 (500000 mẫu) từ portscan\n",
      "Đã tải chunk 10 (500000 mẫu) từ portscan\n",
      "Đã tải chunk 11 (46406 mẫu) từ portscan\n",
      "Đã tải chunk 1 (8549 mẫu) từ revshell_http\n",
      "Đã tải chunk 1 (9404 mẫu) từ revshell_https\n",
      "Đã tải chunk 1 (11353 mẫu) từ smtp_version\n",
      "Đã tải chunk 1 (74300 mẫu) từ sql_injection_http\n",
      "Đã tải chunk 1 (102584 mẫu) từ sql_injection_https\n",
      "Đã tải chunk 1 (34279 mẫu) từ ssh_login\n",
      "Đã tải chunk 1 (34246 mẫu) từ ssh_login_successful\n",
      "Đã tải chunk 1 (5509 mẫu) từ ssrf_http\n",
      "Đã tải chunk 1 (6656 mẫu) từ ssrf_https\n",
      "Đã tải chunk 1 (4558 mẫu) từ xss_http\n",
      "Đã tải chunk 1 (4533 mẫu) từ xss_https\n",
      "\n",
      "=== ĐỒNG BỘ CỘT ===\n",
      "Các cột chung: {'fwd_header_size_min', 'fwd_last_window_size', 'flow_RST_flag_count', 'service', 'attack', 'bwd_pkts_per_sec', 'bwd_last_window_size', 'id.resp_h', 'flow_SYN_flag_count', 'flow_ECE_flag_count', 'flow_duration', 'bwd_header_size_min', 'bwd_header_size_tot', 'fwd_init_window_size', 'fwd_header_size_max', 'flow_pkts_per_sec', 'bwd_data_pkts_tot', 'flow_CWR_flag_count', 'bwd_pkts_tot', 'attack_type', 'uid', 'down_up_ratio', 'fwd_pkts_per_sec', 'fwd_pkts_tot', 'fwd_URG_flag_count', 'bwd_init_window_size', 'fwd_data_pkts_tot', 'ts', 'bwd_PSH_flag_count', 'payload_bytes_per_second', 'traffic_direction', 'id.orig_h', 'flow_FIN_flag_count', 'fwd_header_size_tot', 'fwd_PSH_flag_count', 'bwd_header_size_max', 'bwd_URG_flag_count', 'flow_ACK_flag_count'}\n",
      "Tổng số mẫu: 12059742\n",
      "Các cột có sẵn: ['fwd_header_size_min', 'fwd_last_window_size', 'flow_RST_flag_count', 'service', 'attack', 'bwd_pkts_per_sec', 'bwd_last_window_size', 'id.resp_h', 'flow_SYN_flag_count', 'flow_ECE_flag_count', 'flow_duration', 'bwd_header_size_min', 'bwd_header_size_tot', 'fwd_init_window_size', 'fwd_header_size_max', 'flow_pkts_per_sec', 'bwd_data_pkts_tot', 'flow_CWR_flag_count', 'bwd_pkts_tot', 'attack_type', 'uid', 'down_up_ratio', 'fwd_pkts_per_sec', 'fwd_pkts_tot', 'fwd_URG_flag_count', 'bwd_init_window_size', 'fwd_data_pkts_tot', 'ts', 'bwd_PSH_flag_count', 'payload_bytes_per_second', 'traffic_direction', 'id.orig_h', 'flow_FIN_flag_count', 'fwd_header_size_tot', 'fwd_PSH_flag_count', 'bwd_header_size_max', 'bwd_URG_flag_count', 'flow_ACK_flag_count']\n",
      "\n",
      "=== THÔNG TIN TỔNG QUAN ===\n",
      "Kích thước dữ liệu: (12059742, 38)\n",
      "Số features: 38\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 12059742 entries, 0 to 12059741\n",
      "Data columns (total 38 columns):\n",
      " #   Column                    Dtype  \n",
      "---  ------                    -----  \n",
      " 0   fwd_header_size_min       float64\n",
      " 1   fwd_last_window_size      float64\n",
      " 2   flow_RST_flag_count       float64\n",
      " 3   service                   object \n",
      " 4   attack                    object \n",
      " 5   bwd_pkts_per_sec          float64\n",
      " 6   bwd_last_window_size      float64\n",
      " 7   id.resp_h                 object \n",
      " 8   flow_SYN_flag_count       float64\n",
      " 9   flow_ECE_flag_count       float64\n",
      " 10  flow_duration             float64\n",
      " 11  bwd_header_size_min       float64\n",
      " 12  bwd_header_size_tot       float64\n",
      " 13  fwd_init_window_size      float64\n",
      " 14  fwd_header_size_max       float64\n",
      " 15  flow_pkts_per_sec         float64\n",
      " 16  bwd_data_pkts_tot         float64\n",
      " 17  flow_CWR_flag_count       float64\n",
      " 18  bwd_pkts_tot              float64\n",
      " 19  attack_type               object \n",
      " 20  uid                       object \n",
      " 21  down_up_ratio             float64\n",
      " 22  fwd_pkts_per_sec          float64\n",
      " 23  fwd_pkts_tot              float64\n",
      " 24  fwd_URG_flag_count        float64\n",
      " 25  bwd_init_window_size      float64\n",
      " 26  fwd_data_pkts_tot         float64\n",
      " 27  ts                        object \n",
      " 28  bwd_PSH_flag_count        float64\n",
      " 29  payload_bytes_per_second  float64\n",
      " 30  traffic_direction         object \n",
      " 31  id.orig_h                 object \n",
      " 32  flow_FIN_flag_count       float64\n",
      " 33  fwd_header_size_tot       float64\n",
      " 34  fwd_PSH_flag_count        float64\n",
      " 35  bwd_header_size_max       float64\n",
      " 36  bwd_URG_flag_count        float64\n",
      " 37  flow_ACK_flag_count       float64\n",
      "dtypes: float64(30), object(8)\n",
      "memory usage: 3.4+ GB\n",
      "\n",
      "=== PHÂN PHỐI NHÃN ===\n",
      "attack_type\n",
      "portscan                5046406\n",
      "hostsweep_Pn            3492290\n",
      "bruteforce_http          912503\n",
      "bruteforce_https         865126\n",
      "benign                   825187\n",
      "ftp_login                468275\n",
      "sql_injection_https      102584\n",
      "dos_http                  86443\n",
      "sql_injection_http        74300\n",
      "ssh_login                 34279\n",
      "ssh_login_successful      34246\n",
      "dos_https                 33216\n",
      "hostsweep_sn              22637\n",
      "ftp_version               11688\n",
      "smtp_version              11353\n",
      "revshell_https             9404\n",
      "revshell_http              8549\n",
      "ssrf_https                 6656\n",
      "ssrf_http                  5509\n",
      "xss_http                   4558\n",
      "xss_https                  4533\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Tỷ lệ attack: 93.16%\n",
      "Tỷ lệ benign: 6.84%\n",
      "\n",
      "=== THÔNG TIN MISSING VALUES ===\n",
      "                   Missing Count  Missing Percentage\n",
      "service                  8637989           71.626648\n",
      "traffic_direction           1716            0.014229\n",
      "\n",
      "=== THÔNG TIN KIỂU DỮ LIỆU ===\n",
      "float64    30\n",
      "object      8\n",
      "Name: count, dtype: int64\n",
      "\n",
      "=== THỐNG KÊ SỐ LIỆU ===\n",
      "\n",
      "=== LÀM SẠCH DỮ LIỆU ===\n",
      "\n",
      "=== KIỂM TRA DỮ LIỆU ===\n",
      "\n",
      "=== XỬ LÝ NGOẠI LỆ ===\n",
      "Xử lý ngoại lệ cho cột fwd_header_size_min bằng IQR: lower=24.00, upper=24.00\n",
      "Xử lý ngoại lệ cho cột fwd_last_window_size bằng IQR: lower=-268.50, upper=1799.50\n",
      "Xử lý ngoại lệ cho cột flow_RST_flag_count bằng IQR: lower=-1.50, upper=2.50\n",
      "Xử lý ngoại lệ cho cột bwd_pkts_per_sec bằng IQR: lower=-6304.06, upper=10506.77\n",
      "Xử lý ngoại lệ cho cột bwd_last_window_size bằng IQR: lower=0.00, upper=0.00\n",
      "Xử lý ngoại lệ cho cột flow_SYN_flag_count bằng IQR: lower=1.00, upper=1.00\n",
      "Xử lý ngoại lệ cho cột flow_ECE_flag_count bằng IQR: lower=0.00, upper=0.00\n",
      "Xử lý ngoại lệ cho cột flow_duration bằng IQR: lower=-0.04, upper=0.06\n",
      "Xử lý ngoại lệ cho cột bwd_header_size_min bằng IQR: lower=-30.00, upper=50.00\n",
      "Xử lý ngoại lệ cho cột bwd_header_size_tot bằng IQR: lower=-96.00, upper=160.00\n",
      "Xử lý ngoại lệ cho cột fwd_init_window_size bằng IQR: lower=1024.00, upper=1024.00\n",
      "Xử lý ngoại lệ cho cột fwd_header_size_max bằng IQR: lower=24.00, upper=24.00\n",
      "Xử lý ngoại lệ cho cột flow_pkts_per_sec bằng IQR: lower=-12608.13, upper=21013.55\n",
      "Xử lý ngoại lệ cho cột bwd_data_pkts_tot bằng IQR: lower=-1.50, upper=2.50\n",
      "Xử lý ngoại lệ cho cột flow_CWR_flag_count bằng IQR: lower=0.00, upper=0.00\n",
      "Xử lý ngoại lệ cho cột bwd_pkts_tot bằng IQR: lower=-4.50, upper=7.50\n",
      "Xử lý ngoại lệ cho cột down_up_ratio bằng IQR: lower=-1.50, upper=2.50\n",
      "Xử lý ngoại lệ cho cột fwd_pkts_per_sec bằng IQR: lower=-6304.06, upper=10506.77\n",
      "Xử lý ngoại lệ cho cột fwd_pkts_tot bằng IQR: lower=-5.00, upper=11.00\n",
      "Xử lý ngoại lệ cho cột fwd_URG_flag_count bằng IQR: lower=0.00, upper=0.00\n",
      "Xử lý ngoại lệ cho cột bwd_init_window_size bằng IQR: lower=0.00, upper=0.00\n",
      "Xử lý ngoại lệ cho cột fwd_data_pkts_tot bằng IQR: lower=-1.50, upper=2.50\n",
      "Xử lý ngoại lệ cho cột bwd_PSH_flag_count bằng IQR: lower=0.00, upper=0.00\n",
      "Xử lý ngoại lệ cho cột payload_bytes_per_second bằng IQR: lower=-807.45, upper=1345.74\n",
      "Xử lý ngoại lệ cho cột flow_FIN_flag_count bằng IQR: lower=0.00, upper=0.00\n",
      "Xử lý ngoại lệ cho cột fwd_header_size_tot bằng IQR: lower=-36.00, upper=124.00\n",
      "Xử lý ngoại lệ cho cột fwd_PSH_flag_count bằng IQR: lower=0.00, upper=0.00\n",
      "Xử lý ngoại lệ cho cột bwd_header_size_max bằng IQR: lower=-30.00, upper=50.00\n",
      "Xử lý ngoại lệ cho cột bwd_URG_flag_count bằng IQR: lower=0.00, upper=0.00\n",
      "Xử lý ngoại lệ cho cột flow_ACK_flag_count bằng IQR: lower=-1.50, upper=2.50\n",
      "Đã loại bỏ các cột: ['uid', 'ts', 'id.orig_h', 'id.resp_h', 'service', 'attack']\n",
      "Xử lý missing values cho: ['traffic_direction']\n",
      "Đã loại bỏ 10311091 dòng trùng lặp dựa trên các cột đặc trưng\n",
      "Kích thước sau khi làm sạch: (1748651, 32) (từ (12059742, 38))\n",
      "OneHot encoding cho: ['traffic_direction']\n",
      "Tổng số features sau engineering: 36\n",
      "\n",
      "=== GROUP ATTACK TYPES ===\n",
      "Đã tạo cột 'attack_type_coarse'. Các giá trị: ['benign' 'bruteforce' 'dos' 'ftp' 'reconnaissance' 'reverse_shell' 'smtp'\n",
      " 'sql_injection' 'ssh' 'ssrf' 'xss']\n",
      "\n",
      "=== CHUẨN BỊ FEATURES VÀ LABELS ===\n",
      "Số features: 35\n",
      "Label mapping: {'benign': 0, 'bruteforce': 1, 'dos': 2, 'ftp': 3, 'reconnaissance': 4, 'reverse_shell': 5, 'smtp': 6, 'sql_injection': 7, 'ssh': 8, 'ssrf': 9, 'xss': 10}\n",
      "Shape của X: (1748651, 35)\n",
      "Shape của y: (1748651, 11)\n",
      "Số classes: 11\n",
      "\n",
      "=== RANDOM UNDERSAMPLING + SMOTE ===\n",
      "Target samples per class: 20,000\n",
      "Phân phối trước xử lý:\n",
      "  benign: 483,220\n",
      "  bruteforce: 933,638\n",
      "  dos: 64,563\n",
      "  ftp: 42,251\n",
      "  reconnaissance: 8,398\n",
      "  reverse_shell: 16,642\n",
      "  smtp: 5,474\n",
      "  sql_injection: 119,478\n",
      "  ssh: 54,280\n",
      "  ssrf: 11,797\n",
      "  xss: 8,910\n",
      "  benign: 483,220 → 20,000\n",
      "  bruteforce: 933,638 → 20,000\n",
      "  dos: 64,563 → 20,000\n",
      "  ftp: 42,251 → 20,000\n",
      "  reconnaissance: 8,398 → 20,000\n",
      "  reverse_shell: 16,642 → 20,000\n",
      "  smtp: 5,474 → 20,000\n",
      "  sql_injection: 119,478 → 20,000\n",
      "  ssh: 54,280 → 20,000\n",
      "  ssrf: 11,797 → 20,000\n",
      "  xss: 8,910 → 20,000\n",
      "\n",
      "=== KẾT QUẢ CUỐI CÙNG ===\n",
      "Dataset size: 1,748,651 → 220,000\n",
      "\n",
      "Phân phối cuối cùng:\n",
      "  benign: 20,000 (9.1%)\n",
      "  bruteforce: 20,000 (9.1%)\n",
      "  dos: 20,000 (9.1%)\n",
      "  ftp: 20,000 (9.1%)\n",
      "  reconnaissance: 20,000 (9.1%)\n",
      "  reverse_shell: 20,000 (9.1%)\n",
      "  smtp: 20,000 (9.1%)\n",
      "  sql_injection: 20,000 (9.1%)\n",
      "  ssh: 20,000 (9.1%)\n",
      "  ssrf: 20,000 (9.1%)\n",
      "  xss: 20,000 (9.1%)\n",
      "\n",
      "Balance ratio (min/max): 1.000\n",
      "Dataset rất cân bằng\n",
      "\n",
      "=== CHIA DỮ LIỆU ===\n",
      "Training set: (153999, 35)\n",
      "Validation set: (22000, 35)\n",
      "Test set: (44001, 35)\n",
      "\n",
      "Phân bố nhãn trong các tập:\n",
      "Train: Counter({3: 14000, 4: 14000, 5: 14000, 10: 14000, 0: 14000, 9: 14000, 1: 14000, 7: 14000, 2: 14000, 8: 14000, 6: 13999})\n",
      "Validation: Counter({5: 2000, 3: 2000, 2: 2000, 8: 2000, 1: 2000, 4: 2000, 9: 2000, 10: 2000, 7: 2000, 6: 2000, 0: 2000})\n",
      "Test: Counter({6: 4001, 5: 4000, 3: 4000, 7: 4000, 9: 4000, 2: 4000, 10: 4000, 4: 4000, 8: 4000, 0: 4000, 1: 4000})\n",
      "\n",
      "=== CHUẨN HÓA DỮ LIỆU ===\n",
      "Đã chuẩn hóa dữ liệu:\n",
      "  Train: (153999, 35)\n",
      "  Validation: (22000, 35)\n",
      "  Test: (44001, 35)\n",
      "\n",
      "Đã lưu dữ liệu và cấu hình vào thư mục: webids23_processed_enhanced\n",
      "\n",
      "=== HOÀN THÀNH PIPELINE XỬ LÝ DỮ LIỆU ===\n",
      "\n",
      "Dữ liệu đã sẵn sàng cho training:\n",
      "Training: (153999, 35) features, (153999, 11) labels\n",
      "Validation: (22000, 35) features, (22000, 11) labels\n",
      "Test: (44001, 35) features, (44001, 11) labels\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import LabelEncoder, MinMaxScaler, OneHotEncoder\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.utils import shuffle\n",
    "from imblearn.over_sampling import BorderlineSMOTE\n",
    "from imblearn.under_sampling import TomekLinks, ClusterCentroids\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from sklearn.cluster import MiniBatchKMeans\n",
    "from imblearn.under_sampling import RandomUnderSampler\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from sklearn.metrics import silhouette_score\n",
    "from collections import Counter\n",
    "import warnings\n",
    "import os\n",
    "import pickle\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "class WEBIDS23PreprocessorEnhanced:\n",
    "    \"\"\"\n",
    "    Lớp xử lý dữ liệu WEB-IDS23 cải tiến với batch processing\n",
    "    và sửa lỗi SMOTE trong smote_undersampling_balance\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, chunk_size=500000):\n",
    "        self.label_encoder = LabelEncoder()\n",
    "        self.scaler = MinMaxScaler()\n",
    "        self.onehot_encoder = OneHotEncoder(sparse_output=False, handle_unknown='ignore')\n",
    "        self.feature_columns = []\n",
    "        self.label_mapping = {}\n",
    "        self.chunk_size = chunk_size\n",
    "        self.class_names = [\n",
    "            'benign', 'bruteforce_http', 'bruteforce_https', 'dos_http', 'dos_https',\n",
    "            'ftp_login', 'ftp_version', 'hostsweep_Pn', 'hostsweep_sn', 'portscan',\n",
    "            'revshell_http', 'revshell_https', 'smtp_enum', 'smtp_version',\n",
    "            'sql_injection_http', 'sql_injection_https', 'ssh_login', 'ssh_login_successful',\n",
    "            'ssrf_http', 'ssrf_https', 'xss_http', 'xss_https'\n",
    "        ]\n",
    "        self.coarse_label_mapping = {\n",
    "            'benign': 'benign',\n",
    "            'bruteforce_http': 'bruteforce',\n",
    "            'bruteforce_https': 'bruteforce',\n",
    "            'dos_http': 'dos',\n",
    "            'dos_https': 'dos',\n",
    "            'ftp_login': 'ftp',\n",
    "            'ftp_version': 'ftp',\n",
    "            'hostsweep_Pn': 'reconnaissance',\n",
    "            'hostsweep_sn': 'reconnaissance',\n",
    "            'portscan': 'reconnaissance',\n",
    "            'revshell_http': 'reverse_shell',\n",
    "            'revshell_https': 'reverse_shell',\n",
    "            'smtp_enum': 'smtp',\n",
    "            'smtp_version': 'smtp',\n",
    "            'sql_injection_http': 'sql_injection',\n",
    "            'sql_injection_https': 'sql_injection',\n",
    "            'ssh_login': 'ssh',\n",
    "            'ssh_login_successful': 'ssh',\n",
    "            'ssrf_http': 'ssrf',\n",
    "            'ssrf_https': 'ssrf',\n",
    "            'xss_http': 'xss',\n",
    "            'xss_https': 'xss'\n",
    "        }\n",
    "\n",
    "    def group_attack_types(self, df, col='attack_type'):\n",
    "        \"\"\"Ánh xạ nhãn gốc sang nhãn nhóm\"\"\"\n",
    "        print(f\"\\n=== GROUP ATTACK TYPES ===\")\n",
    "        if col not in df.columns:\n",
    "            raise ValueError(f\"Cột {col} không tồn tại trong DataFrame\")\n",
    "        df['attack_type_coarse'] = df[col].map(self.coarse_label_mapping)\n",
    "        df['attack_type_coarse'] = df['attack_type_coarse'].fillna('other')\n",
    "        df = df.drop(columns=[col])\n",
    "        print(f\"Đã tạo cột 'attack_type_coarse'. Các giá trị: {df['attack_type_coarse'].unique()}\")\n",
    "        return df\n",
    "\n",
    "    def load_data(self, file_paths):\n",
    "        \"\"\"Tải dữ liệu theo chunk từ các file CSV\"\"\"\n",
    "        print(\"Đang tải dữ liệu theo chunk...\")\n",
    "        dataframes = []\n",
    "        if isinstance(file_paths, dict):\n",
    "            for class_name, file_path in file_paths.items():\n",
    "                try:\n",
    "                    chunk_iter = pd.read_csv(file_path, chunksize=self.chunk_size)\n",
    "                    for i, chunk in enumerate(chunk_iter):\n",
    "                        if 'attack_type' not in chunk.columns:\n",
    "                            chunk['attack_type'] = class_name\n",
    "                        dataframes.append(chunk)\n",
    "                        print(f\"Đã tải chunk {i+1} ({len(chunk)} mẫu) từ {class_name}\")\n",
    "                except Exception as e:\n",
    "                    print(f\"Lỗi khi tải file {file_path}: {e}\")\n",
    "        elif isinstance(file_paths, list):\n",
    "            for file_path in file_paths:\n",
    "                try:\n",
    "                    chunk_iter = pd.read_csv(file_path, chunksize=self.chunk_size)\n",
    "                    for i, chunk in enumerate(chunk_iter):\n",
    "                        dataframes.append(chunk)\n",
    "                        print(f\"Đã tải chunk {i+1} ({len(chunk)} mẫu) từ {file_path}\")\n",
    "                except Exception as e:\n",
    "                    print(f\"Lỗi khi tải file {file_path}: {e}\")\n",
    "        if dataframes:\n",
    "            combined_df = pd.concat(dataframes, ignore_index=True)\n",
    "            print(f\"Tổng số mẫu: {len(combined_df)}\")\n",
    "            print(f\"Các cột có sẵn: {list(combined_df.columns)}\")\n",
    "            return combined_df\n",
    "        else:\n",
    "            raise ValueError(\"Không thể tải được dữ liệu từ các file đã cho\")\n",
    "\n",
    "    def explore_data(self, df):\n",
    "        \"\"\"Khám phá và phân tích dữ liệu\"\"\"\n",
    "        print(\"\\n=== THÔNG TIN TỔNG QUAN ===\")\n",
    "        print(f\"Kích thước dữ liệu: {df.shape}\")\n",
    "        print(f\"Số features: {df.shape[1]}\")\n",
    "        df.info()\n",
    "        print(\"\\n=== PHÂN PHỐI NHÃN ===\")\n",
    "        if 'attack_type' in df.columns:\n",
    "            label_counts = df['attack_type'].value_counts()\n",
    "            print(label_counts)\n",
    "            attack_count = (df['attack_type'] != 'benign').sum()\n",
    "            benign_count = (df['attack_type'] == 'benign').sum()\n",
    "            print(f\"\\nTỷ lệ attack: {attack_count/len(df)*100:.2f}%\")\n",
    "            print(f\"Tỷ lệ benign: {benign_count/len(df)*100:.2f}%\")\n",
    "        \n",
    "        print(\"\\n=== THÔNG TIN MISSING VALUES ===\")\n",
    "        missing_info = df.isnull().sum()\n",
    "        missing_percent = (missing_info / len(df)) * 100\n",
    "        missing_df = pd.DataFrame({\n",
    "            'Missing Count': missing_info,\n",
    "            'Missing Percentage': missing_percent\n",
    "        })\n",
    "        print(missing_df[missing_df['Missing Count'] > 0])\n",
    "        \n",
    "        print(\"\\n=== THÔNG TIN KIỂU DỮ LIỆU ===\")\n",
    "        print(df.dtypes.value_counts())\n",
    "        df.describe(include='all')\n",
    "        print(\"\\n=== THỐNG KÊ SỐ LIỆU ===\")\n",
    "        \n",
    "        return df.describe()\n",
    "\n",
    "    def clean_data(self, df):\n",
    "        \"\"\"Làm sạch dữ liệu với median imputation và xử lý trùng lặp có chọn lọc\"\"\"\n",
    "        print(\"\\n=== LÀM SẠCH DỮ LIỆU ===\")\n",
    "        initial_shape = df.shape\n",
    "        \n",
    "        # Loại bỏ các cột không cần thiết\n",
    "        columns_to_drop = ['uid', 'ts', 'id.orig_h', 'id.resp_h', 'service', 'attack']\n",
    "        columns_to_drop = [col for col in columns_to_drop if col in df.columns]\n",
    "        if columns_to_drop:\n",
    "            df = df.drop(columns=columns_to_drop)\n",
    "            print(f\"Đã loại bỏ các cột: {columns_to_drop}\")\n",
    "        \n",
    "        # Median imputation cho cột số, mode imputation cho cột phân loại\n",
    "        missing_cols = df.columns[df.isnull().any()].tolist()\n",
    "        if missing_cols:\n",
    "            print(f\"Xử lý missing values cho: {missing_cols}\")\n",
    "            numeric_cols = df.select_dtypes(include=[np.number]).columns\n",
    "            if numeric_cols.any():\n",
    "                for col in numeric_cols.intersection(missing_cols):\n",
    "                    df[col] = df[col].fillna(df[col].median())\n",
    "            for col in missing_cols:\n",
    "                if col not in numeric_cols:\n",
    "                    df[col] = df[col].fillna(df[col].mode()[0])\n",
    "        \n",
    "        # Loại bỏ duplicate dựa trên các cột đặc trưng chính, giữ lại một số mẫu\n",
    "        duplicates = df.duplicated(subset=[col for col in df.columns if col not in ['attack_type', 'attack']]).sum()\n",
    "        if duplicates > 0:\n",
    "            df = df.drop_duplicates(subset=[col for col in df.columns if col not in ['attack_type', 'attack']], keep='first')\n",
    "            print(f\"Đã loại bỏ {duplicates} dòng trùng lặp dựa trên các cột đặc trưng\")\n",
    "        \n",
    "        # Xử lý infinite values\n",
    "        numeric_cols = df.select_dtypes(include=[np.number]).columns\n",
    "        df[numeric_cols] = df[numeric_cols].replace([np.inf, -np.inf], np.nan)\n",
    "        df[numeric_cols] = df[numeric_cols].fillna(df[numeric_cols].median())\n",
    "        \n",
    "        print(f\"Kích thước sau khi làm sạch: {df.shape} (từ {initial_shape})\")\n",
    "        return df\n",
    "\n",
    "    def feature_engineering(self, df):\n",
    "        \n",
    "        # # Đặc trưng thời gian\n",
    "        # if 'total_pkts' in df.columns and 'flow_duration' in df.columns:\n",
    "        #     df['packets_per_second'] = df['total_pkts'] / (df['flow_duration'] + 1)\n",
    "        # else:\n",
    "        #     print(\"Cảnh báo: Thiếu cột 'total_pkts' hoặc 'flow_duration'. Bỏ qua đặc trưng packets_per_second.\")\n",
    "        \n",
    "        # # TCP flags\n",
    "        # flag_cols = [c for c in df.columns if 'flag_count' in c]\n",
    "        # for c in flag_cols:\n",
    "        #     df[f'{c}_bin'] = (df[c] > 0).astype(int)\n",
    "        #     if 'total_pkts' in df.columns:\n",
    "        #         df[f'{c}_ratio'] = df[c] / (df['total_pkts'] + 1)\n",
    "        \n",
    "        # OneHotEncoder cho các cột phân loại\n",
    "        categorical_cols = df.select_dtypes(include=['object']).columns.tolist()\n",
    "        for col in ['attack', 'attack_type', 'attack_type_coarse']:\n",
    "            if col in categorical_cols:\n",
    "                categorical_cols.remove(col)\n",
    "        if categorical_cols:\n",
    "            print(f\"OneHot encoding cho: {categorical_cols}\")\n",
    "            encoded_data = self.onehot_encoder.fit_transform(df[categorical_cols])\n",
    "            encoded_cols = self.onehot_encoder.get_feature_names_out(categorical_cols)\n",
    "            encoded_df = pd.DataFrame(encoded_data, columns=encoded_cols, index=df.index)\n",
    "            df = df.drop(columns=categorical_cols)\n",
    "            df = pd.concat([df, encoded_df], axis=1)\n",
    "        \n",
    "        print(f\"Tổng số features sau engineering: {df.shape[1]}\")\n",
    "        return df\n",
    "    \n",
    "    def prepare_features_labels(self, df, target_col='attack_type_coarse'):\n",
    "        \"\"\"Chuẩn bị features và labels\"\"\"\n",
    "        print(\"\\n=== CHUẨN BỊ FEATURES VÀ LABELS ===\")\n",
    "        if target_col not in df.columns:\n",
    "            raise ValueError(f\"Cột target {target_col} không tồn tại trong DataFrame. Các cột có sẵn: {list(df.columns)}\")\n",
    "        X = df.drop(columns=[target_col, 'attack'], errors='ignore')\n",
    "        y = df[target_col]\n",
    "        self.feature_columns = X.columns.tolist()\n",
    "        print(f\"Số features: {len(self.feature_columns)}\")\n",
    "        y_encoded = self.label_encoder.fit_transform(y)\n",
    "        self.label_mapping = dict(zip(self.label_encoder.classes_, \n",
    "                                    self.label_encoder.transform(self.label_encoder.classes_)))\n",
    "        print(f\"Label mapping: {self.label_mapping}\")\n",
    "        y_categorical = to_categorical(y_encoded)\n",
    "        print(f\"Shape của X: {X.shape}\")\n",
    "        print(f\"Shape của y: {y_categorical.shape}\")\n",
    "        print(f\"Số classes: {y_categorical.shape[1]}\")\n",
    "        return X, y_categorical, y_encoded\n",
    "\n",
    "    def balance_data_random_smote(self, X, y, target_samples=20000, k_neighbors=5):\n",
    "        \"\"\"\n",
    "        Cân bằng dữ liệu với RandomUnderSampler cho lớp lớn và SMOTE cho lớp nhỏ.\n",
    "        \n",
    "        Parameters:\n",
    "        -----------\n",
    "        X : array-like, shape (n_samples, n_features)\n",
    "            Dữ liệu đầu vào\n",
    "        y : array-like, shape (n_samples,) hoặc one-hot encoded\n",
    "            Nhãn của dữ liệu\n",
    "        target_samples : int, optional (default=20000)\n",
    "            Số mẫu mục tiêu cho mỗi lớp\n",
    "        k_neighbors : int, optional (default=5)\n",
    "            Số láng giềng cho SMOTE\n",
    "        \n",
    "        Returns:\n",
    "        --------\n",
    "        X_balanced : array, shape (n_samples_balanced, n_features)\n",
    "            Dữ liệu đã cân bằng\n",
    "        y_balanced_onehot : array, shape (n_samples_balanced, n_classes)\n",
    "            Nhãn one-hot encoded sau khi cân bằng\n",
    "        \"\"\"\n",
    "        print(f\"\\n=== RANDOM UNDERSAMPLING + SMOTE ===\")\n",
    "        print(f\"Target samples per class: {target_samples:,}\")\n",
    "\n",
    "        # Chuẩn bị dữ liệu\n",
    "        X_array = X.values if hasattr(X, 'values') else X\n",
    "        y_labels = y.argmax(axis=1) if y.ndim > 1 else y\n",
    "        n_samples = X_array.shape[0]\n",
    "\n",
    "        # Chuẩn hóa dữ liệu\n",
    "        scaler = MinMaxScaler()\n",
    "        X_scaled = scaler.fit_transform(X_array)\n",
    "\n",
    "        # Đếm phân phối lớp ban đầu\n",
    "        class_counts = Counter(y_labels)\n",
    "        print(\"Phân phối trước xử lý:\")\n",
    "        for class_idx, count in sorted(class_counts.items()):\n",
    "            class_name = self.label_encoder.inverse_transform([class_idx])[0]\n",
    "            print(f\"  {class_name}: {count:,}\")\n",
    "\n",
    "        X_balanced = []\n",
    "        y_balanced = []\n",
    "\n",
    "        for class_idx in sorted(class_counts.keys()):\n",
    "            class_indices = np.where(y_labels == class_idx)[0]\n",
    "            current_count = len(class_indices)\n",
    "            effective_target = 20000\n",
    "\n",
    "            if current_count > effective_target:\n",
    "                # RandomUnderSampler cho lớp lớn\n",
    "                rus = RandomUnderSampler(sampling_strategy={class_idx: effective_target}, \n",
    "                                    random_state=42)\n",
    "                X_resampled, y_resampled = rus.fit_resample(X_scaled, y_labels)\n",
    "                selected_indices = np.where(y_resampled == class_idx)[0]\n",
    "                X_balanced.append(X_resampled[selected_indices])\n",
    "                y_balanced.extend([class_idx] * effective_target)\n",
    "            else:\n",
    "                # SMOTE cho lớp nhỏ\n",
    "                smote = SMOTE(sampling_strategy={class_idx: effective_target},\n",
    "                            k_neighbors=min(k_neighbors, max(1, current_count-1)),\n",
    "                            random_state=42)\n",
    "                X_resampled, y_resampled = smote.fit_resample(X_scaled, y_labels)\n",
    "                selected_indices = np.where(y_resampled == class_idx)[0]\n",
    "                X_balanced.append(X_resampled[selected_indices])\n",
    "                y_balanced.extend([class_idx] * effective_target)\n",
    "\n",
    "            class_name = self.label_encoder.inverse_transform([class_idx])[0]\n",
    "            print(f\"  {class_name}: {current_count:,} → {effective_target:,}\")\n",
    "\n",
    "        # Gộp dữ liệu\n",
    "        X_balanced = np.vstack(X_balanced)\n",
    "        y_balanced = np.array(y_balanced)\n",
    "\n",
    "        # Khôi phục về không gian gốc\n",
    "        X_balanced = scaler.inverse_transform(X_balanced)\n",
    "        X_balanced = np.clip(X_balanced, a_min=0, a_max=None)\n",
    "\n",
    "        # Chuyển đổi nhãn sang one-hot encoding\n",
    "        y_balanced_onehot = to_categorical(y_balanced, num_classes=len(self.label_mapping))\n",
    "\n",
    "        # In kết quả cuối cùng\n",
    "        final_counts = Counter(y_balanced)\n",
    "        print(f\"\\n=== KẾT QUẢ CUỐI CÙNG ===\")\n",
    "        print(f\"Dataset size: {n_samples:,} → {len(X_balanced):,}\")\n",
    "        print(\"\\nPhân phối cuối cùng:\")\n",
    "        total_samples = len(X_balanced)\n",
    "        for class_idx, count in sorted(final_counts.items()):\n",
    "            class_name = self.label_encoder.inverse_transform([class_idx])[0]\n",
    "            percentage = (count / total_samples) * 100\n",
    "            print(f\"  {class_name}: {count:,} ({percentage:.1f}%)\")\n",
    "\n",
    "        # Đánh giá mức độ cân bằng\n",
    "        counts_array = np.array(list(final_counts.values()))\n",
    "        balance_ratio = counts_array.min() / counts_array.max()\n",
    "        print(f\"\\nBalance ratio (min/max): {balance_ratio:.3f}\")\n",
    "        if balance_ratio > 0.9:\n",
    "            print(\"Dataset rất cân bằng\")\n",
    "        elif balance_ratio > 0.8:\n",
    "            print(\"Dataset khá cân bằng\")\n",
    "        else:\n",
    "            print(\"Dataset chưa hoàn toàn cân bằng\")\n",
    "\n",
    "        return X_balanced, y_balanced_onehot\n",
    "    def scale_features(self, X_train, X_val, X_test):\n",
    "        \"\"\"\n",
    "        Chuẩn hóa các tập dữ liệu train, val, test sử dụng MinMaxScaler.\n",
    "        \n",
    "        Parameters:\n",
    "        -----------\n",
    "        X_train : array-like, shape (n_samples_train, n_features)\n",
    "            Dữ liệu train\n",
    "        X_val : array-like, shape (n_samples_val, n_features)\n",
    "            Dữ liệu validation\n",
    "        X_test : array-like, shape (n_samples_test, n_features)\n",
    "            Dữ liệu test\n",
    "        \n",
    "        Returns:\n",
    "        --------\n",
    "        X_train_scaled : array, shape (n_samples_train, n_features)\n",
    "            Dữ liệu train đã chuẩn hóa\n",
    "        X_val_scaled : array, shape (n_samples_val, n_features)\n",
    "            Dữ liệu validation đã chuẩn hóa\n",
    "        X_test_scaled : array, shape (n_samples_test, n_features)\n",
    "            Dữ liệu test đã chuẩn hóa\n",
    "        \"\"\"\n",
    "        print(\"\\n=== CHUẨN HÓA DỮ LIỆU ===\")\n",
    "        \n",
    "        # Chuyển đổi sang numpy array nếu là DataFrame\n",
    "        X_train_array = X_train.values if hasattr(X_train, 'values') else X_train\n",
    "        X_val_array = X_val.values if hasattr(X_val, 'values') else X_val\n",
    "        X_test_array = X_test.values if hasattr(X_test, 'values') else X_test\n",
    "        \n",
    "        # Fit scaler trên tập train và transform trên cả 3 tập\n",
    "        self.scaler.fit(X_train_array)\n",
    "        X_train_scaled = self.scaler.transform(X_train_array)\n",
    "        X_val_scaled = self.scaler.transform(X_val_array)\n",
    "        X_test_scaled = self.scaler.transform(X_test_array)\n",
    "        \n",
    "        print(f\"Đã chuẩn hóa dữ liệu:\")\n",
    "        print(f\"  Train: {X_train_scaled.shape}\")\n",
    "        print(f\"  Validation: {X_val_scaled.shape}\")\n",
    "        print(f\"  Test: {X_test_scaled.shape}\")\n",
    "        \n",
    "        return X_train_scaled, X_val_scaled, X_test_scaled\n",
    "    def split_data(self, X, y, test_size=0.2, val_size=0.1, random_state=42):\n",
    "        \"\"\"Chia dữ liệu thành train/val/test\"\"\"\n",
    "        print(\"\\n=== CHIA DỮ LIỆU ===\")\n",
    "        X, y = shuffle(X, y, random_state=random_state)\n",
    "        try:\n",
    "            X_train, X_temp, y_train, y_temp = train_test_split(\n",
    "                X, y, test_size=test_size + val_size, \n",
    "                random_state=random_state, stratify=y.argmax(axis=1)\n",
    "            )\n",
    "            relative_val_size = val_size / (test_size + val_size)\n",
    "            X_val, X_test, y_val, y_test = train_test_split(\n",
    "                X_temp, y_temp, test_size=1-relative_val_size,\n",
    "                random_state=random_state, stratify=y_temp.argmax(axis=1)\n",
    "            )\n",
    "        except ValueError as e:\n",
    "            print(f\"Lỗi stratified split: {e}\")\n",
    "            print(\"Chuyển sang random split...\")\n",
    "            X_train, X_temp, y_train, y_temp = train_test_split(\n",
    "                X, y, test_size=test_size + val_size, random_state=random_state\n",
    "            )\n",
    "            relative_val_size = val_size / (test_size + val_size)\n",
    "            X_val, X_test, y_val, y_test = train_test_split(\n",
    "                X_temp, y_temp, test_size=1-relative_val_size, random_state=random_state\n",
    "            )\n",
    "        \n",
    "        print(f\"Training set: {X_train.shape}\")\n",
    "        print(f\"Validation set: {X_val.shape}\")\n",
    "        print(f\"Test set: {X_test.shape}\")\n",
    "        print(\"\\nPhân bố nhãn trong các tập:\")\n",
    "        print(\"Train:\", Counter(y_train.argmax(axis=1)))\n",
    "        print(\"Validation:\", Counter(y_val.argmax(axis=1)))\n",
    "        print(\"Test:\", Counter(y_test.argmax(axis=1)))\n",
    "        \n",
    "        return X_train, X_val, X_test, y_train, y_val, y_test\n",
    "\n",
    "    def save_processed_data(self, X_train, X_val, X_test, y_train, y_val, y_test, \n",
    "                           save_dir='processed_data'):\n",
    "        \"\"\"Lưu dữ liệu đã xử lý\"\"\"\n",
    "        os.makedirs(save_dir, exist_ok=True)\n",
    "        np.save(f'{save_dir}/X_train.npy', X_train)\n",
    "        np.save(f'{save_dir}/X_val.npy', X_val)\n",
    "        np.save(f'{save_dir}/X_test.npy', X_test)\n",
    "        np.save(f'{save_dir}/y_train.npy', y_train)\n",
    "        np.save(f'{save_dir}/y_val.npy', y_val)\n",
    "        np.save(f'{save_dir}/y_test.npy', y_test)\n",
    "        metadata = {\n",
    "            'feature_columns': self.feature_columns,\n",
    "            'label_mapping': self.label_mapping,\n",
    "            'num_classes': len(self.label_mapping),\n",
    "            'num_features': len(self.feature_columns)\n",
    "        }\n",
    "        with open(f'{save_dir}/metadata.pkl', 'wb') as f:\n",
    "            pickle.dump(metadata, f)\n",
    "        with open(f'{save_dir}/scaler.pkl', 'wb') as f:\n",
    "            pickle.dump(self.scaler, f)\n",
    "        with open(f'{save_dir}/onehot_encoder.pkl', 'wb') as f:\n",
    "            pickle.dump(self.onehot_encoder, f)\n",
    "        print(f\"\\nĐã lưu dữ liệu xử lý vào thư mục: {save_dir}\")\n",
    "\n",
    "    def full_preprocessing_pipeline(self, file_paths, use_smote_undersampling=True,\n",
    "                                  target_samples_per_class=2000, k_neighbors=5,\n",
    "                                  save_data=True, save_dir='processed_data'):\n",
    "        \"\"\"Pipeline xử lý dữ liệu hoàn chỉnh với batch processing\"\"\"\n",
    "        print(\"=== BẮT ĐẦU PIPELINE XỬ LÝ DỮ LIỆU ===\")\n",
    "        try:\n",
    "            df = self.load_data(file_paths)\n",
    "            self.explore_data(df)\n",
    "            df = self.clean_data(df)\n",
    "            df = self.feature_engineering(df)\n",
    "            df = self.group_attack_types(df, col='attack_type')\n",
    "            X, y, y_encoded = self.prepare_features_labels(df, target_col='attack_type_coarse')\n",
    "            if use_smote_undersampling:\n",
    "                X, y = self.balance_data_random_smote(X, y, target_samples_per_class, k_neighbors)\n",
    "            X_train, X_val, X_test, y_train, y_val, y_test = self.split_data(X, y)\n",
    "            X_train_scaled, X_val_scaled, X_test_scaled = self.scale_features(X_train, X_val, X_test)\n",
    "            df.head(15)\n",
    "            if save_data:\n",
    "                self.save_processed_data(\n",
    "                    X_train_scaled, X_val_scaled, X_test_scaled,\n",
    "                    y_train, y_val, y_test, save_dir\n",
    "                )\n",
    "            print(\"\\n=== HOÀN THÀNH PIPELINE XỬ LÝ DỮ LIỆU ===\")\n",
    "            return X_train_scaled, X_val_scaled, X_test_scaled, y_train, y_val, y_test\n",
    "        except Exception as e:\n",
    "            print(f\"\\nLỗi trong pipeline: {e}\")\n",
    "            print(\"\\nGợi ý khắc phục:\")\n",
    "            print(\"1. Kiểm tra đường dẫn file có đúng không\")\n",
    "            print(\"2. Thử tăng chunk_size (ví dụ: 1000000)\")\n",
    "            print(\"3. Thử giảm target_samples_per_class xuống (ví dụ: 1000)\")\n",
    "            print(\"4. Đặt use_smote_undersampling=False để không balance\")\n",
    "            print(\"5. Kiểm tra các cột trong dữ liệu bằng log từ load_data và feature_engineering\")\n",
    "            raise\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    preprocessor = WEBIDS23PreprocessorEnhanced(chunk_size=500000)\n",
    "    file_paths = {\n",
    "        'benign': r\"D:\\Download\\dataverse_files\\web-ids23\\web-ids23_benign.csv\",\n",
    "        'bruteforce_http': r\"D:\\Download\\dataverse_files\\web-ids23\\web-ids23_bruteforce_http.csv\",\n",
    "        'bruteforce_https': r\"D:\\Download\\dataverse_files\\web-ids23\\web-ids23_bruteforce_https.csv\",\n",
    "        'dos_http': r\"D:\\Download\\dataverse_files\\web-ids23\\web-ids23_dos_http.csv\",\n",
    "        'dos_https': r\"D:\\Download\\dataverse_files\\web-ids23\\web-ids23_dos_https.csv\",\n",
    "        'ftp_login': r\"D:\\Download\\dataverse_files\\web-ids23\\web-ids23_ftp_login.csv\",\n",
    "        'ftp_version': r\"D:\\Download\\dataverse_files\\web-ids23\\web-ids23_ftp_version.csv\",\n",
    "        'hostsweep_Pn': r\"D:\\Download\\dataverse_files\\web-ids23\\web-ids23_hostsweep_Pn.csv\",\n",
    "        'hostsweep_sn': r\"D:\\Download\\dataverse_files\\web-ids23\\web-ids23_hostsweep_sn.csv\",\n",
    "        'portscan': r\"D:\\Download\\dataverse_files\\web-ids23\\web-ids23_portscan.csv\",\n",
    "        'revshell_http': r\"D:\\Download\\dataverse_files\\web-ids23\\web-ids23_revshell_http.csv\",\n",
    "        'revshell_https': r\"D:\\Download\\dataverse_files\\web-ids23\\web-ids23_revshell_https.csv\",\n",
    "        # 'smtp_enum': r\"D:\\Download\\dataverse_files\\web-ids23\\web-ids23_smtp_enum.csv\",\n",
    "        'smtp_version': r\"D:\\Download\\dataverse_files\\web-ids23\\web-ids23_smtp_version.csv\",\n",
    "        'sql_injection_http': r\"D:\\Download\\dataverse_files\\web-ids23\\web-ids23_sql_injection_http.csv\",\n",
    "        'sql_injection_https': r\"D:\\Download\\dataverse_files\\web-ids23\\web-ids23_sql_injection_https.csv\",\n",
    "        'ssh_login': r\"D:\\Download\\dataverse_files\\web-ids23\\web-ids23_ssh_login.csv\",\n",
    "        'ssh_login_successful': r\"D:\\Download\\dataverse_files\\web-ids23\\web-ids23_ssh_login_successful.csv\",\n",
    "        'ssrf_http': r\"D:\\Download\\dataverse_files\\web-ids23\\web-ids23_ssrf_http.csv\",\n",
    "        'ssrf_https': r\"D:\\Download\\dataverse_files\\web-ids23\\web-ids23_ssrf_https.csv\",\n",
    "        'xss_http': r\"D:\\Download\\dataverse_files\\web-ids23\\web-ids23_xss_http.csv\",\n",
    "        'xss_https': r\"D:\\Download\\dataverse_files\\web-ids23\\web-ids23_xss_https.csv\",\n",
    "    }\n",
    "    try:\n",
    "        X_train, X_val, X_test, y_train, y_val, y_test = preprocessor.full_preprocessing_pipeline(\n",
    "            file_paths=file_paths,\n",
    "            use_smote_undersampling=True,\n",
    "            target_samples_per_class=20000,\n",
    "            k_neighbors=5,\n",
    "            save_data=True,\n",
    "            save_dir='webids23_processed_enhanced'\n",
    "        )\n",
    "        print(f\"\\nDữ liệu đã sẵn sàng cho training:\")\n",
    "        print(f\"Training: {X_train.shape} features, {y_train.shape} labels\")\n",
    "        print(f\"Validation: {X_val.shape} features, {y_val.shape} labels\")\n",
    "        print(f\"Test: {X_test.shape} features, {y_test.shape} labels\")\n",
    "    except Exception as e:\n",
    "        print(f\"Lỗi trong quá trình xử lý: {e}\")\n",
    "        print(\"Vui lòng kiểm tra đường dẫn file và định dạng dữ liệu.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tf",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.23"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
